# Summary

Developing and implementing different algorithmics to solve several optimization problems.

# Optimization Test Problems

## 1. Gradient Descent Method

An implementation of the Gradient Descent method developed to solve the famous [Rosenbrock Function](https://www.sfu.ca/~ssurjano/rosen.html).

## 2. Newton's method

An implementation of the Newton's method developed to solve the famous [Rosenbrock Function](https://www.sfu.ca/~ssurjano/rosen.html).

## 3. Karush–Kuhn–Tucker conditions

An implementation of the [Karush–Kuhn–Tucker](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) conditions to solve a minimization problem subject to inequality constraints. 

## 4. Penalty Method

An implementation of a [Penalty method](https://en.wikipedia.org/wiki/Penalty_method), developed as a [logarithmic barrier function](https://en.wikipedia.org/wiki/Barrier_function) to solve a minimization problem subject to inequality constraints.
